---
title: "Jasen_Regularization"
author: "Jasen Zhang"
date: "11/21/2020"
output: pdf_document
---

# 1 Loading the Data

```{r}
library(tidyverse)
library(gridExtra)
library(gplots)
library(glmnet)

dir = getwd()
data_dir <- paste('df_imp.RData', sep = '')
load(data_dir)

response_vars <- c('TARGET_deathRate')

df <- df_imp3 %>% select(- c('ID', 'incidenceRate'))


response_vars <- c('TARGET_deathRate')
vars <- colnames(df)
vars_1 <- setdiff(vars, response_vars)
predict_vars <- paste(vars_1, collapse = ' + ')

df <- data.frame(sapply(df, as.numeric))
```


# 2 Splitting Test and Training Set

## Arbitrarily chose 10% to be test set

```{r message=FALSE}
set.seed(221)
test_set_index <- sample(1:nrow(df), floor(nrow(df))/10)
train_set_index <- setdiff(1:nrow(df), test_set_index)

test <- df[test_set_index,]
train <- df[train_set_index,]

test_y <- test %>% select(response_vars)
test_x <- test %>% select(vars_1)
train_y <- train %>% select(response_vars)
train_x <- train %>% select(vars_1)
```

# 3 Lasso + Leave one out Cross Validation

## 10 folds since that's default

```{r}

lambda_seq <- 10^seq(-2, -1, by = .05)

cv_output <- cv.glmnet(as.matrix(train_x), as.matrix(train_y),
                       alpha = 1, lambda = lambda_seq)

best_lambda <- cv_output$lambda.min

lasso_best <- glmnet(as.matrix(train_x), as.matrix(train_y), alpha = 1, lambda = best_lambda)

pred <- predict(lasso_best, s = best_lambda, newx = as.matrix(test_x))
pred_train_y <- predict(lasso_best, s = best_lambda, newx = as.matrix(train_x))

```

# 3.5 Plotting Graph of Hyperparameter

```{r}
hyper <- cv_output$cvm
original_hyper <- hyper
rescaled_hyper <- (hyper-min(hyper))/(max(hyper)-min(hyper))
hyper <- data.frame(lambda_seq, rescaled_hyper )

g_hyper <- ggplot(data = hyper, aes(x = lambda_seq, y = original_hyper)) + geom_point() + 
  scale_x_continuous(trans = 'log10')

g_hyper
```

# 4 Plotting Y_hat vs Y for training set

```{r}
train_results <- data.frame(train_y, pred_train_y)
colnames(train_results) <- c('y_true', 'y_hat')
R_squared <- as.numeric(unname(cor(train_y, pred_train_y)))
R_squared <- sprintf("%.3f", round(R_squared,3))
R_squared_label <- paste('R^2 = ', R_squared)

g <- ggplot(train_results) + 
  geom_point(aes(x = y_true, y= y_hat)) + 
  geom_line(aes(x = y_true, y = y_true))  +
  geom_label(label = R_squared_label, x = 100, y = 275, label.padding = unit(0.55, "lines"),
    label.size = 0.35,
    color = "black",
    fill="#69b3a2")

g
```

```{r}
sqrt(sum((train_results$y_true - train_results$y_hat)^2)/nrow(train_results))
```

# 5 Plotting Y_hat vs Y for test set

```{r}
test_results <- data.frame(test_y, pred)
colnames(test_results) <- c('y_true', 'y_hat')
R_squared <- as.numeric(unname(cor(test_y, pred)))
R_squared <- sprintf("%.3f", round(R_squared,3))
R_squared_label <- paste('R^2 = ', R_squared)

g <- ggplot(test_results) + 
  geom_point(aes(x = y_true, y= y_hat)) + 
  geom_line(aes(x = y_true, y = y_true))  +
  geom_label(label = R_squared_label, x = 125, y = 250, label.padding = unit(0.55, "lines"),
    label.size = 0.35,
    color = "black",
    fill="#69b3a2")

g
```


```{r}
sqrt(sum((test_results$y_true - test_results$y_hat)^2)/nrow(test_results))
```

